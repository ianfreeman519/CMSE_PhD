#!/bin/bash --login

# This script is submitted using the 'sbatch' command, i.e., 
#    'sbatch example_batch_script.sb'  (without the quotes).
#
# Look at https://wiki.hpcc.msu.edu/display/ITH/Job+Management+by+SLURM for
# job management commands (to see and modify submitted jobs)

########## SBATCH Lines for Resource Request ##########
#
# See https://slurm.schedmd.com/sbatch.html for a much more complete listing of options.
# 
# See https://docs.nersc.gov/jobs/examples/ for examples of node/task/cpu/etc. configurations
# (and https://help.rc.ufl.edu/doc/Sample_SLURM_Scripts for additional examples)
 
#SBATCH --time=05:59:00             # limit of wall clock time - how long the job will run (same as -t)
#SBATCH --nodes=1                  # number of different nodes - could be an exact number or a range of nodes (same as -N)
#SBATCH --ntasks=1                 # number of tasks - how many tasks (nodes) that you require (same as -n)
#SBATCH --cpus-per-task=1          # number of CPUs (or cores) per task (same as -c)
#SBATCH --mem-per-cpu=2G            # memory required per allocated CPU (or core) - amount of memory (in bytes)
#SBATCH --job-name scrapeMinP        # you can give your job a name for easier identification (same as -J)
#SBATCH --mail-user=freem386@msu.edu   # lists the email address to which emails are sent, with options defined by --mail-type
#SBATCH --mail-type=ALL             # let me know when jobs start, end, or fail.

 
########## Command Lines to Run ##########
module purge
source ~/.bashrc
conda activate research

python plot3dmin.py

# WE WILL TEST ON 64 CORES / NODES
scontrol show job $SLURM_JOB_ID     ### write job information to output file

